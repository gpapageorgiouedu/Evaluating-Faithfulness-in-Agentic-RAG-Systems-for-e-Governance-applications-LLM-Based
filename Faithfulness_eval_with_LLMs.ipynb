{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjaZw4RnWKsL"
   },
   "source": [
    "#### Pip and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/gpapageorgiouedu/Evaluating-Faithfulness-in-Agentic-RAG-Systems-for-e-Governance-applications-LLM-Based.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPwLo_NrOuLA"
   },
   "outputs": [],
   "source": [
    "# Important: Please restart the session update the packages install\n",
    "\n",
    "!pip install -q \\\n",
    "neo4j-haystack==2.2.1 \\\n",
    "anthropic-haystack==3.1.0 \\\n",
    "google-ai-haystack==5.3.0 \\\n",
    "openai==1.72.0 \\\n",
    "sentence-transformers==3.4.1 \\\n",
    "yfiles_jupyter_graphs==1.10.2 \\\n",
    "trafilatura==2.0.0 \\\n",
    "demjson3==3.0.6 \\\n",
    "tiktoken==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TCf9E-_Blak"
   },
   "outputs": [],
   "source": [
    "# core libs imports for data handling, file management, and type annotations\n",
    "import json\n",
    "import demjson3\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import shutil\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "\n",
    "# google colab utils for output and secure data storage\n",
    "from google.colab import output, userdata\n",
    "from google.colab.output import eval_js\n",
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "\n",
    "# neo4j database integration\n",
    "from neo4j import GraphDatabase, basic_auth\n",
    "\n",
    "# haystack/ neo4j integration components\n",
    "from neo4j_haystack import Neo4jDocumentStore, Neo4jEmbeddingRetriever\n",
    "\n",
    "# graph visualization in notebooks (explanatory)\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "\n",
    "# haystack components for pipeline construction and data processing\n",
    "from haystack import Pipeline, component, Document\n",
    "from haystack.core.component import component, Component\n",
    "from haystack.components.agents import Agent\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.converters import HTMLToDocument\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack_integrations.components.generators.anthropic import AnthropicChatGenerator\n",
    "from haystack_integrations.components.generators.google_ai import GoogleAIGeminiChatGenerator\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.components.rankers import TransformersSimilarityRanker\n",
    "from haystack.components.websearch import SerperDevWebSearch\n",
    "from haystack.components.evaluators import FaithfulnessEvaluator, ContextRelevanceEvaluator\n",
    "from haystack.core import SuperComponent\n",
    "from haystack.dataclasses import ChatMessage, ToolCall, ToolCallResult, TextContent\n",
    "from haystack.tools.component_tool import ComponentTool\n",
    "from haystack.utils import Secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjVf-8QzOuPo"
   },
   "outputs": [],
   "source": [
    "# set environment variables from secure colab userdata and read environment variables into local constants\n",
    "os.environ[\"NEO4J_URI\"] = userdata.get(\"NEO4J_URI\")\n",
    "os.environ[\"NEO4J_USERNAME\"] = userdata.get(\"NEO4J_USERNAME\")\n",
    "os.environ[\"NEO4J_PASSWORD\"] = userdata.get(\"NEO4J_PASSWORD\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI\")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\"ANTHROPIC\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GEMINI\")\n",
    "os.environ[\"SERPERDEV_API_KEY\"] = userdata.get(\"SERPER\")\n",
    "\n",
    "NEO4J_URI = os.environ[\"NEO4J_URI\"]\n",
    "NEO4J_USER = os.environ[\"NEO4J_USERNAME\"]\n",
    "NEO4J_PASS = os.environ[\"NEO4J_PASSWORD\"]\n",
    "SERPERDEV_API_KEY = os.environ[\"SERPERDEV_API_KEY\"]\n",
    "\n",
    "# init and test neo4j connection\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))\n",
    "try:\n",
    "    with driver.session() as session:\n",
    "        info = session.run(\"RETURN 1 AS result\").single()\n",
    "        print(\"Neo4j connected, test query result:\", info[\"result\"])\n",
    "finally:\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIAcD3G4C5xM"
   },
   "source": [
    "#### Delete Docs (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijiLK5kWCpWz"
   },
   "outputs": [],
   "source": [
    "def delete_all(tx):\n",
    "    \"\"\"\n",
    "    Delete all nodes and relationships from the graph.\n",
    "\n",
    "    Args:\n",
    "        tx: Neo4j transaction object.\n",
    "    \"\"\"\n",
    "    tx.run(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "\n",
    "def count_remaining(tx):\n",
    "    \"\"\"\n",
    "    Count remaining nodes in the graph.\n",
    "\n",
    "    Args:\n",
    "        tx: Neo4j transaction object.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of nodes remaining.\n",
    "    \"\"\"\n",
    "    result = tx.run(\"MATCH (n) RETURN count(n) AS node_count\")\n",
    "    return result.single()[\"node_count\"]\n",
    "\n",
    "\n",
    "def list_node_labels(tx):\n",
    "    \"\"\"\n",
    "    List all unique labels in the graph.\n",
    "\n",
    "    Args:\n",
    "        tx: Neo4j transaction object.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of label names.\n",
    "    \"\"\"\n",
    "    result = tx.run(\"CALL db.labels()\")\n",
    "    return [record[\"label\"] for record in result]\n",
    "\n",
    "\n",
    "def count_by_label(tx, label):\n",
    "    \"\"\"\n",
    "    Count the number of nodes for a specific label.\n",
    "\n",
    "    Args:\n",
    "        tx: Neo4j transaction object.\n",
    "        label (str): The label to count.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of nodes with the given label.\n",
    "    \"\"\"\n",
    "    result = tx.run(f\"MATCH (n:`{label}`) RETURN count(n) AS count\")\n",
    "    return result.single()[\"count\"]\n",
    "\n",
    "\n",
    "# display node counts grouped by label\n",
    "with driver.session() as session:\n",
    "    labels = session.read_transaction(list_node_labels)\n",
    "    for label in labels:\n",
    "        count = session.read_transaction(count_by_label, label)\n",
    "        print(f\"Label: {label}, Count: {count}\")\n",
    "\n",
    "\n",
    "def force_delete_by_labels(tx, labels):\n",
    "    \"\"\"\n",
    "    Forcefully delete nodes by specified labels.\n",
    "\n",
    "    Args:\n",
    "        tx: Neo4j transaction object.\n",
    "        labels (List[str]): A list of node labels to delete.\n",
    "    \"\"\"\n",
    "    for label in labels:\n",
    "        tx.run(f\"MATCH (n:`{label}`) DETACH DELETE n\")\n",
    "\n",
    "\n",
    "# delete all nodes by label, and verify complete deletion\n",
    "with driver.session() as session:\n",
    "    labels = session.read_transaction(list_node_labels)\n",
    "    session.write_transaction(force_delete_by_labels, labels)\n",
    "\n",
    "with driver.session() as session:\n",
    "    session.write_transaction(delete_all)\n",
    "    remaining = session.read_transaction(count_remaining)\n",
    "\n",
    "    if remaining == 0:\n",
    "        print(\"All nodes and relationships successfully deleted.\")\n",
    "    else:\n",
    "        print(f\"Deletion incomplete: {remaining} node(s) still exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giPAmjbuC_om"
   },
   "source": [
    "#### Index Docs (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dE6wa4ZB8JU9"
   },
   "outputs": [],
   "source": [
    "# load docs from your drive, otherwise direct to local repo/ upload\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "src_folder = '/content/drive/My Drive/folder' # direct it into your folder or make a manual upload\n",
    "dst_folder = 'json_docs'\n",
    "\n",
    "os.makedirs(dst_folder, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(src_folder):\n",
    "    src_file = os.path.join(src_folder, filename)\n",
    "    dst_file = os.path.join(dst_folder, filename)\n",
    "    if os.path.isfile(src_file):\n",
    "        shutil.copy(src_file, dst_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XPHYIsaOuWt"
   },
   "outputs": [],
   "source": [
    "def load_json_documents(json_folder_path):\n",
    "    \"\"\"\n",
    "    Load JSON files from a folder and convert them into Haystack Document objects.\n",
    "\n",
    "    Args:\n",
    "        json_folder_path (str or Path): Path to the folder containing .json files.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of Haystack Document objects with content and metadata.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for json_file in Path(json_folder_path).glob(\"*.json\"):\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "            # standardize metadata keys -> add any fits to your data use case\n",
    "            if \"url\" in data:\n",
    "                data[\"source_url\"] = data.pop(\"source_url\")\n",
    "            if \"date\" in data:\n",
    "                data[\"date\"] = data.pop(\"date\")\n",
    "            if \"title\" in data:\n",
    "                data[\"title\"] = data.pop(\"title\")\n",
    "\n",
    "            content = data.get(\"content\", \"\")\n",
    "            metadata = {k: v for k, v in data.items() if k != \"content\"}\n",
    "\n",
    "            documents.append(Document(content=content, meta=metadata))\n",
    "    return documents\n",
    "\n",
    "\n",
    "# load raw documents from a local folder\n",
    "raw_docs = load_json_documents(\"json_docs\")\n",
    "\n",
    "# clean the documents before embedding\n",
    "cleaner = DocumentCleaner(\n",
    "    remove_empty_lines=True,\n",
    "    remove_extra_whitespaces=True,\n",
    "    remove_substrings=[\"...\"]\n",
    ")\n",
    "cleaned_docs = cleaner.run(raw_docs)[\"documents\"]\n",
    "\n",
    "# init neo4j document store for embedding storage\n",
    "document_store = Neo4jDocumentStore(\n",
    "    url=NEO4J_URI, username=NEO4J_USER, password=NEO4J_PASS,\n",
    "    database=\"neo4j\",\n",
    "    index=\"document-embeddings\",\n",
    "    embedding_field=\"embedding\",\n",
    "    embedding_dim=1536,\n",
    "    node_label=\"Document\"\n",
    ")\n",
    "\n",
    "# generate embeddings (ada-002 in our use case)\n",
    "embedder = OpenAIDocumentEmbedder(model=\"text-embedding-ada-002\")\n",
    "documents_with_emb = embedder.run(cleaned_docs)[\"documents\"]\n",
    "\n",
    "# index the embedded documents in the neo4j store\n",
    "document_store.write_documents(documents_with_emb)\n",
    "print(f\"Indexed {document_store.count_documents()} documents in Neo4j.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wik4q7aoGF6o"
   },
   "outputs": [],
   "source": [
    "def count_tokens(text, model=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a given text using the specified model's tokenizer.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text to tokenize.\n",
    "        model (str): The model name to determine which tokenizer to use. Defaults to \"gpt-4.1-mini\".\n",
    "\n",
    "    Returns:\n",
    "        int: The number of tokens in the input text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(model)\n",
    "    except Exception:\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(enc.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_XqCjB7EAna"
   },
   "outputs": [],
   "source": [
    "def evaluate_triples_with_gpt_metrics(text, extracted_triples, doc_id, model=\"gpt-4.1\"):\n",
    "    system_prompt = (\n",
    "        \"You are a rigorous information extraction evaluator. You receive a passage of text and a set of knowledge triples. \"\n",
    "        \"For each triple, determine if it is 'Correct' (supported by the text) or 'Hallucinated' (not fully supported by the text). \"\n",
    "        \"Give a brief explanation for each. If there are factual triples in the text that are missing from the set, list them as 'missed_triples'. \"\n",
    "        \"Return your evaluation as JSON in this format: \"\n",
    "        \"{ 'results': [ {'triple': {...}, 'evaluation': 'Correct' or 'Hallucinated', 'explanation': '...' } ], 'missed_triples': [ {...}, ... ] }\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"Text:\\n{text}\\n\\nExtracted Triples:\\n{json.dumps(extracted_triples, indent=2)}\"\n",
    "\n",
    "    # count input tokens\n",
    "    input_tokens = count_tokens(system_prompt + user_prompt, model)\n",
    "\n",
    "    # start timer\n",
    "    start_time = time.perf_counter()\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    latency = time.perf_counter() - start_time\n",
    "\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    try:\n",
    "        evaluation = json.loads(content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error parsing evaluation response:\", e)\n",
    "        print(\"Raw response was:\\n\", content)\n",
    "        return None\n",
    "\n",
    "    evaluation['doc_id'] = doc_id\n",
    "    evaluation['input_tokens'] = input_tokens\n",
    "    evaluation['latency_seconds'] = latency\n",
    "\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlUCCyVoOubM"
   },
   "outputs": [],
   "source": [
    "def extract_structured_triples_with_metrics(text_chunk, doc_id, model=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    Extract structured knowledge triples from a given text using an OpenAI language model.\n",
    "\n",
    "    The output is a list of dictionaries, where each dictionary represents a triple with:\n",
    "    - 'head': subject of the triple\n",
    "    - 'head_type': type/classification of the head\n",
    "    - 'relation': relationship between head and tail (in UPPER_SNAKE_CASE)\n",
    "    - 'tail': object of the triple\n",
    "    - 'tail_type': type/classification of the tail\n",
    "\n",
    "    Args:\n",
    "        text_chunk (str): A passage of text from which to extract triples.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of structured triples, or an empty list on failure.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are an information extraction assistant.\"\n",
    "        \"You are an expert in European Union's news, policies, laws, and actions. \"\n",
    "        \"Extract all factual knowledge triples from the text in a structured format. \"\n",
    "        \"Return the results as a JSON list where each item is an object with the keys: \"\n",
    "        \"'head', 'head_type', 'relation', 'tail', 'tail_type'.\\n\\n\"\n",
    "        \"Guidelines:\\n\"\n",
    "        \"Resolve vague pronouns (like 'I', 'we', 'they', 'he/she') to actual entities based on context.\\n\"\n",
    "        \"Use the standard full format, even when abbreviations are used in the text. For example, when 'EU' is used, write it as 'European Union'.\"\n",
    "        \"Use the standard full format for names, even if the full name is not used entirely in a specific sentence.\"\n",
    "        \"If provide information please include the full context on triples.\\n\"\n",
    "        \"Maintain consistency: refer to entities by their full and most complete identifiers.\\n\"\n",
    "        \"Use concise relation phrases written in UPPER_SNAKE_CASE.\\n\"\n",
    "        \"Avoid vague, incomplete, or uninformative triples. Use full context to provide informative and comprehensive triples.\\n\"\n",
    "        \"Return only the JSON list of objects. Do not include any explanations, additional knowledge, or markdown.\\n\"\n",
    "        \"If an entity type is unclear, make a reasonable guess or use a general type like 'Entity'.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"Text: ```{text_chunk}```\"\n",
    "    # token count for full input\n",
    "    input_tokens = count_tokens(system_prompt + user_prompt, model)\n",
    "\n",
    "    # start timer\n",
    "    start_time = time.perf_counter()\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    latency = time.perf_counter() - start_time\n",
    "\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    content = re.sub(r\"^```json\\n?\", \"\", content)\n",
    "    content = re.sub(r\"\\n?```$\", \"\", content)\n",
    "\n",
    "    try:\n",
    "        structured_triples = json.loads(content)\n",
    "    except json.JSONDecodeError as json_err:\n",
    "        print(\"JSON decoding error:\", json_err)\n",
    "        print(\"Raw output was:\\n\", content[:500])\n",
    "        structured_triples = []\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        structured_triples = []\n",
    "\n",
    "    # return with metrics and doc_id\n",
    "    return {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"triples\": structured_triples,\n",
    "        \"latency_seconds\": latency,\n",
    "        \"input_tokens\": input_tokens,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFecTMJMtvvr"
   },
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "    try:\n",
    "        session.run(\"CREATE FULLTEXT INDEX entity_index IF NOT EXISTS FOR (n) ON EACH [n.id]\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0tHgyvwOudM"
   },
   "outputs": [],
   "source": [
    "def sanitize_label(label):\n",
    "    \"\"\"\n",
    "    Sanitize a string to be a valid Neo4j label.\n",
    "\n",
    "    Converts non-alphanumeric characters to underscores and ensures the first letter is uppercase.\n",
    "    Returns a default value if the label is empty after sanitization.\n",
    "\n",
    "    Args:\n",
    "        label (str): The label string to sanitize.\n",
    "\n",
    "    Returns:\n",
    "        str: A sanitized, Neo4j-safe label.\n",
    "    \"\"\"\n",
    "    label = re.sub(r\"[^a-zA-Z0-9]\", \"_\", label.strip())\n",
    "    if not label:\n",
    "        return \"Entity\"\n",
    "    return label[0].upper() + label[1:]\n",
    "\n",
    "\n",
    "def retry_on_json_error(fn, max_retries=10, wait_sec=1):\n",
    "    \"\"\"\n",
    "    Retry a function multiple times when JSON-related errors or empty results occur.\n",
    "\n",
    "    This utility repeatedly calls the provided function until it returns a non-None \n",
    "    result or the maximum number of retries is reached. It is useful when dealing with \n",
    "    unreliable JSON responses (e.g., decoding failures).\n",
    "\n",
    "    Args:\n",
    "        fn (Callable): The function to call. It should return a non-None value \n",
    "            on success, or None on failure (e.g., due to JSON decode issues).\n",
    "        max_retries (int, optional): The maximum number of attempts before giving up. \n",
    "            Defaults to 10.\n",
    "        wait_sec (int or float, optional): The number of seconds to wait between attempts. \n",
    "            Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        Any: The first non-None result returned by `fn`.  \n",
    "        None: If all retries fail or return None.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = fn()\n",
    "            if result is not None:\n",
    "                return result\n",
    "            else:\n",
    "                print(f\"Attempt {attempt+1} returned None (likely JSON decode failure or empty result)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} failed with exception: {e}\")\n",
    "        time.sleep(wait_sec)\n",
    "    print(f\"All {max_retries} attempts failed.\")\n",
    "    return None\n",
    "\n",
    "os.makedirs(\"triple_extraction_results\", exist_ok=True)\n",
    "os.makedirs(\"triple_evaluation_results\", exist_ok=True)\n",
    "\n",
    "extraction_path = \"triple_extraction_results/all_extractions.jsonl\"\n",
    "evaluation_path = \"triple_evaluation_results/all_evaluations.jsonl\"\n",
    "\n",
    "# clear files if they exist (for clean run)\n",
    "open(extraction_path, \"w\").close()\n",
    "open(evaluation_path, \"w\").close()\n",
    "\n",
    "with driver.session() as session:\n",
    "    try:\n",
    "        session.run(\"CREATE FULLTEXT INDEX entity_index IF NOT EXISTS FOR (n) ON EACH [n.id]\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not create index:\", e)\n",
    "\n",
    "with driver.session() as session:\n",
    "    for doc in documents_with_emb:\n",
    "        doc_id = doc.id\n",
    "        text = doc.content\n",
    "\n",
    "        # extraction with retry\n",
    "        def do_extraction():\n",
    "            return extract_structured_triples_with_metrics(text, doc_id)\n",
    "        extraction = retry_on_json_error(do_extraction)\n",
    "        if extraction is None:\n",
    "            print(f\"Extraction failed for doc {doc_id}, skipping.\")\n",
    "            continue\n",
    "        triples = extraction['triples']\n",
    "\n",
    "        # Neo4j indexing\n",
    "        for triple in triples:\n",
    "            subj = triple.get(\"head\")\n",
    "            subj_type = sanitize_label(triple.get(\"head_type\", \"Entity\"))\n",
    "            pred = triple.get(\"relation\")\n",
    "            obj = triple.get(\"tail\")\n",
    "            obj_type = sanitize_label(triple.get(\"tail_type\", \"Entity\"))\n",
    "            if not subj or not pred or not obj:\n",
    "                continue\n",
    "            rel_type = \"_\".join(pred.strip().split()).upper()\n",
    "            rel_type = re.sub(r\"[^A-Z0-9_]\", \"_\", rel_type)\n",
    "            cypher = f\"\"\"\n",
    "            MERGE (s:{subj_type} {{id: $subj}})\n",
    "            MERGE (o:{obj_type} {{id: $obj}})\n",
    "            MERGE (s)-[r:{rel_type}]->(o)\n",
    "            MERGE (d:Document {{id: $doc_id}})\n",
    "            MERGE (d)-[:MENTIONS]->(s)\n",
    "            MERGE (d)-[:MENTIONS]->(o)\n",
    "            \"\"\"\n",
    "            session.run(cypher, {\n",
    "                \"subj\": subj,\n",
    "                \"obj\": obj,\n",
    "                \"doc_id\": doc_id\n",
    "            })\n",
    "\n",
    "        # save extraction dynamically (append as a line)\n",
    "        with open(extraction_path, \"a\") as f:\n",
    "            f.write(json.dumps(extraction) + \"\\n\")\n",
    "\n",
    "        print(\n",
    "            f\"Doc {doc_id}: {len(triples)} triples, \"\n",
    "            f\"{extraction['latency_seconds']:.2f}s, \"\n",
    "            f\"{extraction['input_tokens']} tokens\"\n",
    "        )\n",
    "\n",
    "        # evaluation with retry\n",
    "        def do_evaluation():\n",
    "            return evaluate_triples_with_gpt_metrics(text, triples, doc_id)\n",
    "        evaluation = retry_on_json_error(do_evaluation)\n",
    "        if evaluation:\n",
    "            # save evaluation dynamically (append as a line)\n",
    "            with open(evaluation_path, \"a\") as f:\n",
    "                f.write(json.dumps(evaluation) + \"\\n\")\n",
    "            print(\n",
    "                f\"Eval {doc_id}: {len(evaluation.get('results', []))} evaluated, \"\n",
    "                f\"{evaluation['latency_seconds']:.2f}s, \"\n",
    "                f\"{evaluation['input_tokens']} tokens\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Evaluation failed for doc {doc_id}\")\n",
    "\n",
    "\n",
    "# convert extractions\n",
    "with open(\"triple_extraction_results/all_extractions.jsonl\") as fin:\n",
    "    all_extractions = [json.loads(line) for line in fin]\n",
    "\n",
    "with open(\"triple_extraction_results/all_extractions.json\", \"w\") as fout:\n",
    "    json.dump(all_extractions, fout, indent=2)\n",
    "\n",
    "# convert evaluations\n",
    "with open(\"triple_evaluation_results/all_evaluations.jsonl\") as fin:\n",
    "    all_evaluations = [json.loads(line) for line in fin]\n",
    "\n",
    "with open(\"triple_evaluation_results/all_evaluations.json\", \"w\") as fout:\n",
    "    json.dump(all_evaluations, fout, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlBA-fAZDKtu"
   },
   "source": [
    "#### RAG Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pHc-aFuh-EK"
   },
   "outputs": [],
   "source": [
    "@component\n",
    "class KnowledgeGraphRetriever():\n",
    "    \"\"\"\n",
    "    A custom Haystack component for retrieving context-rich documents from a Neo4j knowledge graph\n",
    "    based on search terms extracted by an OpenAI model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, neo4j_uri: str, neo4j_user: str, neo4j_pass: str, openai_model=\"gpt-4.1-mini\"):\n",
    "        self._driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_pass))\n",
    "        self._model = openai_model\n",
    "\n",
    "    @component.output_types(documents=List[Document])\n",
    "    def run(self, query: str) -> Dict[str, List[Document]]:\n",
    "        \"\"\"\n",
    "        Run retrieval based on an input query.\n",
    "\n",
    "        Uses a language model to extract search terms, runs Cypher queries against Neo4j,\n",
    "        and formats the results into Haystack Document objects.\n",
    "\n",
    "        Args:\n",
    "            query (str): The natural language query from the user.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List[Document]]: A dictionary with a single key \"documents\" containing the result set.\n",
    "        \"\"\"\n",
    "        system_prompt = (\n",
    "            \"You are a search term extractor. Based on the user question, return a list of 1–10 keywords or named entities \"\n",
    "            \"that should be used to search a knowledge graph. Use lowercase, and return only a clean list in JSON like [\\\"term1\\\", \\\"term2\\\"]\"\n",
    "        )\n",
    "\n",
    "        response = openai.chat.completions.create(\n",
    "            model=self._model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        raw_content = response.choices[0].message.content.strip()\n",
    "        try:\n",
    "            terms = re.findall(r'\"(.*?)\"', raw_content)\n",
    "            if not terms:\n",
    "                terms = [query]\n",
    "        except Exception:\n",
    "            terms = [query]\n",
    "\n",
    "        documents = []\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for term in terms:\n",
    "                cypher = \"\"\"\n",
    "                    MATCH (n)-[r]-(connected)\n",
    "                    WHERE toLower(n.id) CONTAINS toLower($query)\n",
    "                    OPTIONAL MATCH (n)<-[:MENTIONS]-(d:Document)\n",
    "                    OPTIONAL MATCH (connected)<-[:MENTIONS]-(d2:Document)\n",
    "                    RETURN n, r, connected, coalesce(d, d2) AS doc\n",
    "                \"\"\"\n",
    "                result = session.run(cypher, {\"query\": term})\n",
    "\n",
    "                grouped_output = defaultdict(lambda: {\"to_doc\": [], \"other\": []})\n",
    "                doc_text_lookup = {}\n",
    "\n",
    "                for record in result:\n",
    "                    n = record[\"n\"]\n",
    "                    r = record[\"r\"]\n",
    "                    connected = record[\"connected\"]\n",
    "                    doc_node = record.get(\"doc\")\n",
    "\n",
    "                    n_label = list(n.labels)[0] if n.labels else \"Entity\"\n",
    "                    connected_label = list(connected.labels)[0] if connected.labels else \"Entity\"\n",
    "                    n_id = n.get(\"id\", \"[no-id]\")\n",
    "\n",
    "                    if doc_node:\n",
    "                        doc_id = doc_node.get(\"id\", \"unknown\")\n",
    "                        doc_content = doc_node.get(\"content\", \"[No content]\")\n",
    "                        doc_title = doc_node.get(\"title\", \"[No Title]\")\n",
    "                        doc_url = doc_node.get(\"source_url\", \"[No URL]\")\n",
    "                        doc_date = doc_node.get(\"date\", \"[No Date]\")\n",
    "                        full_doc_text = f\"Title: {doc_title}\\nDate: {doc_date}\\nURL: {doc_url}\\n\\n{doc_content}\"\n",
    "                        doc_text_lookup[doc_id] = full_doc_text\n",
    "                    else:\n",
    "                        doc_id = \"no_doc\"\n",
    "\n",
    "                    is_connected_doc = \"Document\" in connected.labels\n",
    "\n",
    "                    if is_connected_doc:\n",
    "                        triple_line = f\"({n_label}: {n_id}) -[{r.type}]-> In Document below:\"\n",
    "                        grouped_output[doc_id][\"to_doc\"].append(triple_line)\n",
    "                    else:\n",
    "                        connected_value = connected.get(\"id\", \"[no-id]\")\n",
    "                        triple_line = f\"({n_label}: {n_id}) -[{r.type}]-> ({connected_label}: {connected_value})\"\n",
    "                        grouped_output[doc_id][\"other\"].append(triple_line)\n",
    "\n",
    "                for doc_id, groups in grouped_output.items():\n",
    "                    doc_lines = groups[\"to_doc\"]\n",
    "                    other_lines = groups[\"other\"]\n",
    "                    content_parts = []\n",
    "\n",
    "                    if doc_lines:\n",
    "                        content_parts.extend(doc_lines)\n",
    "                        doc_text = doc_text_lookup.get(doc_id, \"[No document content]\")\n",
    "                        content_parts.append(\"\\n\\nDocument:\\n\" + doc_text)\n",
    "\n",
    "                    if other_lines:\n",
    "                        content_parts.append(\"\")\n",
    "                        content_parts.extend(other_lines)\n",
    "\n",
    "                    final_content = \"\\n\\n\".join(content_parts).strip()\n",
    "                    meta = {\"source_doc_id\": doc_id} if doc_id != \"no_doc\" else {}\n",
    "                    documents.append(Document(content=final_content, meta=meta))\n",
    "\n",
    "        if not documents:\n",
    "            documents.append(Document(content=\"(No results found)\"))\n",
    "\n",
    "        return {\"documents\": documents}\n",
    "\n",
    "@component\n",
    "class DocumentPassthrough:\n",
    "    def run(self, documents: List[Document]) -> Dict[str, List[Document]]:\n",
    "        return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsPnXC9l-pM4"
   },
   "outputs": [],
   "source": [
    "# config the neo4j document store for retrieval\n",
    "document_store = Neo4jDocumentStore(\n",
    "    url=NEO4J_URI, username=NEO4J_USER, password=NEO4J_PASS,\n",
    "    database=\"neo4j\",\n",
    "    index=\"document-embeddings\",\n",
    "    embedding_field=\"embedding\",\n",
    "    embedding_dim=1536,\n",
    "    node_label=\"Document\"\n",
    ")\n",
    "\n",
    "# init components\n",
    "embedder_emb = OpenAITextEmbedder(model=\"text-embedding-ada-002\", api_key= Secret.from_env_var(\"OPENAI_API_KEY\"))\n",
    "\n",
    "retriever_emb = Neo4jEmbeddingRetriever(document_store=document_store)\n",
    "\n",
    "ranker_emb = TransformersSimilarityRanker(\n",
    "    model=\"intfloat/simlm-msmarco-reranker\", top_k=5\n",
    ")\n",
    "\n",
    "prompt_template_emb = \"\"\"\n",
    "You are an AI Assistant with access to official Documents about the European Union's news, policies, laws, and actions.\n",
    "\n",
    "Your task is to answer user questions **STRICTLY** based on the Documents provided below.\n",
    "\n",
    "Question: {{ query }}\n",
    "\n",
    "Guidelines:\n",
    "- Use only the content from the provided  Documents.\n",
    "- Do NOT rely on prior or external knowledge.\n",
    "- Answer strictly by **copying**, **quoting**, or **paraphrasing** only what is written in the provided Documents.\n",
    "- **Never generate information, conclusions, or details that are not explicitly present in the documents.**\n",
    "- Do NOT ask the user for additional information.\n",
    "- Include inline HTML links for referencing URL sources in the answer, using the URLs provided in the Documents.\n",
    "  - Use the document’s title as the anchor text.\n",
    "  - If the title is missing, use the domain name of the document’s URL as the anchor text.\n",
    "- Each fact you refer to should be followed by the corresponding reference.\n",
    "- Output the answer in a structured markdown format.\n",
    "- Use bullet lists whenever it makes sense.\n",
    "- Do not add a references section at the end of the answer, just use references within the body of text.\n",
    "\n",
    "If a definitive answer cannot be found in the Documents, respond with:\n",
    "Final Answer: inconclusive\n",
    "\n",
    "Always end your answer with this disclaimer:\n",
    "Disclaimer: This is AI generated content — please use it with caution.\n",
    "\n",
    "Documents:\n",
    "{% for doc in documents %}\n",
    "Source: <a href=\"{{ doc.meta.source_url }}\"</a><br>\n",
    "Title: <a href=\"{{ doc.meta.title }}\"</a><br>\n",
    "Date: <a href=\"{{ doc.meta.date }}\"</a><br>\n",
    "\n",
    "{{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder_emb = PromptBuilder(\n",
    "    template=prompt_template_emb,\n",
    "    required_variables=[\"documents\", \"query\"]\n",
    ")\n",
    "\n",
    "generator_emb = OpenAIGenerator(\n",
    "    model=\"gpt-4.1-mini\", api_key=Secret.from_env_var(\"OPENAI_API_KEY\"),\n",
    "    generation_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0\n",
    "    }\n",
    ")\n",
    "\n",
    "# create the pipeline, register components and connect pipeline components\n",
    "emb_pipeline = Pipeline()\n",
    "emb_pipeline.add_component(\"embedder\", embedder_emb)\n",
    "emb_pipeline.add_component(\"retriever\", retriever_emb)\n",
    "emb_pipeline.add_component(\"reranker\", ranker_emb)\n",
    "emb_pipeline.add_component(\"output_docs\", DocumentPassthrough())\n",
    "emb_pipeline.add_component(\"prompt_builder\", prompt_builder_emb)\n",
    "emb_pipeline.add_component(\"generator\", generator_emb)\n",
    "\n",
    "emb_pipeline.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n",
    "emb_pipeline.connect(\"retriever.documents\", \"reranker.documents\")\n",
    "emb_pipeline.connect(\"reranker.documents\", \"output_docs.documents\")\n",
    "emb_pipeline.connect(\"reranker.documents\", \"prompt_builder.documents\")\n",
    "emb_pipeline.connect(\"prompt_builder\", \"generator\")\n",
    "\n",
    "# prepare pipeline for use\n",
    "emb_pipeline.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJDGVqoXNRly"
   },
   "outputs": [],
   "source": [
    "# init the knowledge graph retriever component\n",
    "kg_retriever = KnowledgeGraphRetriever(\n",
    "    neo4j_uri=NEO4J_URI,\n",
    "    neo4j_user=NEO4J_USER,\n",
    "    neo4j_pass=NEO4J_PASS\n",
    ")\n",
    "\n",
    "# Set up reranker for refining retrieved graph-based documents\n",
    "ranker_graph = TransformersSimilarityRanker(\n",
    "    model=\"intfloat/simlm-msmarco-reranker\", top_k=5\n",
    ")\n",
    "\n",
    "# Define prompt template tailored for graph-based documents\n",
    "prompt_template_graph = \"\"\"\n",
    "You are an AI Assistant with access to official Knowledge Graphs and Documents about the European Union's news, policies, laws, and actions.\n",
    "\n",
    "Your task is to answer user questions **STRICTLY** based on the Knowledge Graphs and Documents provided below.\n",
    "\n",
    "Generate your answer based on the presented extracted **Triples**, using the Document passage as supplementary information, without making any assumptions.\n",
    "\n",
    "Question: {{ query }}\n",
    "\n",
    "Guidelines:\n",
    "- Use only the content from the provided Knowledge Graphs and Documents.\n",
    "- Do NOT rely on prior or external knowledge.\n",
    "- Answer strictly by **copying**, **quoting**, or **paraphrasing** only what is written in the provided Knowledge Graphs and Documents.\n",
    "- **Never generate information, conclusions, or details that are not explicitly present in the documents.**\n",
    "- Do NOT ask the user for additional information.\n",
    "- Include inline HTML links for referencing URL sources in the answer, using the URLs provided in the Knowledge Graphs and Documents.\n",
    "  - Use the document’s title as the anchor text.\n",
    "  - If the title is missing, use the domain name of the document’s URL as the anchor text.\n",
    "- Each fact you refer to should be followed by the corresponding reference.\n",
    "- Output the answer in a structured markdown format.\n",
    "- Use bullet lists whenever it makes sense.\n",
    "- Do not add a references section at the end of the answer, just use references within the body of text.\n",
    "\n",
    "If a definitive answer cannot be found in the Knowledge Graphs and Documents, respond with:\n",
    "Final Answer: inconclusive\n",
    "\n",
    "Always end your answer with this disclaimer:\n",
    "Disclaimer: This is AI generated content — please use it with caution.\n",
    "\n",
    "Knowledge Graphs and Documents:\n",
    "\n",
    "Each source below may contain both structured (graph/triple) information and unstructured (document/text) content.\n",
    "\n",
    "{% for doc in documents %}\n",
    "- <b>Source:</b><br>\n",
    "{{ doc.content }}<br><br>\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}\n",
    "\n",
    "Answer (with references using HTML links):\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder_graph = PromptBuilder(\n",
    "    template=prompt_template_graph,\n",
    "    required_variables=[\"documents\", \"query\"]\n",
    ")\n",
    "\n",
    "# init generator\n",
    "generator_graph = OpenAIGenerator(\n",
    "    model=\"gpt-4.1-mini\", api_key=Secret.from_env_var(\"OPENAI_API_KEY\"),\n",
    "    generation_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0\n",
    "    }\n",
    ")\n",
    "\n",
    "# Build the knowledge graph pipeline\n",
    "graph_pipeline = Pipeline()\n",
    "graph_pipeline.add_component(\"kg_retriever\", kg_retriever)\n",
    "graph_pipeline.add_component(\"ranker\", ranker_graph)\n",
    "graph_pipeline.add_component(\"output_docs\", DocumentPassthrough())\n",
    "graph_pipeline.add_component(\"prompt_builder\", prompt_builder_graph)\n",
    "graph_pipeline.add_component(\"generator\", generator_graph)\n",
    "\n",
    "# Define the flow of data between components\n",
    "graph_pipeline.connect(\"kg_retriever.documents\", \"ranker.documents\")\n",
    "graph_pipeline.connect(\"ranker.documents\", \"output_docs.documents\")\n",
    "graph_pipeline.connect(\"ranker.documents\", \"prompt_builder.documents\")\n",
    "graph_pipeline.connect(\"prompt_builder\", \"generator\")\n",
    "\n",
    "# Prepare the pipeline\n",
    "graph_pipeline.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7532GE2FXQj"
   },
   "outputs": [],
   "source": [
    "# define components for web based retrieval and generation\n",
    "web_search = SerperDevWebSearch(top_k=5, api_key=Secret.from_env_var(\"SERPERDEV_API_KEY\"))\n",
    "fetcher = LinkContentFetcher()\n",
    "converter = HTMLToDocument()\n",
    "ranker_web = TransformersSimilarityRanker(model=\"intfloat/simlm-msmarco-reranker\", top_k=5)\n",
    "\n",
    "# prompt template with required variables for generation\n",
    "prompt_template_web = \"\"\"\n",
    "You are an AI Assistant with access to Web Search Documents about the European Union's news, policies, laws, and actions.\n",
    "\n",
    "Your task is to answer user questions **STRICTLY** based on the Documents provided below.\n",
    "\n",
    "Question: {{ query }}\n",
    "\n",
    "Guidelines:\n",
    "- Use only the content from the provided  Documents.\n",
    "- Do NOT rely on prior or external knowledge.\n",
    "- Answer strictly by **copying**, **quoting**, or **paraphrasing** only what is written in the provided Documents.\n",
    "- **Never generate information, conclusions, or details that are not explicitly present in the documents.**\n",
    "- Do NOT ask the user for additional information.\n",
    "- Include inline HTML links for referencing URL sources in the answer, using the URLs provided in the Documents.\n",
    "  - Use the document’s title as the anchor text.\n",
    "  - If the title is missing, use the domain name of the document’s URL as the anchor text.\n",
    "- Each fact you refer to should be followed by the corresponding reference.\n",
    "- Output the answer in a structured markdown format.\n",
    "- Use bullet lists whenever it makes sense.\n",
    "- Do not add a references section at the end of the answer, just use references within the body of text.\n",
    "\n",
    "If a definitive answer cannot be found in the Documents, respond with:\n",
    "Final Answer: inconclusive\n",
    "\n",
    "Always end your answer with this disclaimer:\n",
    "Disclaimer: This is AI generated content — please use it with caution.\n",
    "\n",
    "Documents:\n",
    "{% for doc in documents %}\n",
    "- <b>Source:</b> <a href=\"{{ doc.meta.url }}\">{{ doc.meta.url }}</a><br>\n",
    "<p>{{ doc.content }}</p><br>\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder_web = PromptBuilder(\n",
    "    template=prompt_template_web,\n",
    "    required_variables=[\"documents\", \"query\"]\n",
    ")\n",
    "\n",
    "generator_web = OpenAIGenerator(\n",
    "    model=\"gpt-4.1-mini\", api_key=Secret.from_env_var(\"OPENAI_API_KEY\"),\n",
    "    generation_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0\n",
    "    }\n",
    ")\n",
    "\n",
    "# create, config the web search pipeline and connect the components\n",
    "web_pipeline = Pipeline()\n",
    "web_pipeline.add_component(\"search\", web_search)\n",
    "web_pipeline.add_component(\"fetcher\", fetcher)\n",
    "web_pipeline.add_component(\"converter\", converter)\n",
    "web_pipeline.add_component(\"ranker\", ranker_web)\n",
    "web_pipeline.add_component(\"output_docs\", DocumentPassthrough())\n",
    "web_pipeline.add_component(\"prompt_builder\", prompt_builder_web)\n",
    "web_pipeline.add_component(\"generator\", generator_web)\n",
    "\n",
    "web_pipeline.connect(\"search.links\", \"fetcher.urls\")\n",
    "web_pipeline.connect(\"fetcher.streams\", \"converter.sources\")\n",
    "web_pipeline.connect(\"converter.documents\", \"ranker.documents\")\n",
    "web_pipeline.connect(\"ranker.documents\", \"output_docs.documents\")\n",
    "web_pipeline.connect(\"ranker.documents\", \"prompt_builder.documents\")\n",
    "web_pipeline.connect(\"prompt_builder\", \"generator\")\n",
    "\n",
    "# prepare the pipeline for inference\n",
    "web_pipeline.warm_up()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyJjq0u0QJt0"
   },
   "source": [
    "#### Agent Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbuFqXdcNRp1"
   },
   "outputs": [],
   "source": [
    "# init the chat generator for multi-turn interaction\n",
    "chat_generator = OpenAIChatGenerator(\n",
    "    model=\"gpt-4.1\",\n",
    "    api_key=Secret.from_env_var(\"OPENAI_API_KEY\"),\n",
    "    generation_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0\n",
    "    }\n",
    ")\n",
    "\n",
    "# wrap the pipelines with input/output mappings\n",
    "graph_super = SuperComponent(\n",
    "    pipeline=graph_pipeline,\n",
    "    input_mapping={\n",
    "        \"query\": [\"kg_retriever.query\", \"ranker.query\", \"prompt_builder.query\"]\n",
    "    },\n",
    "    output_mapping={\"generator.replies\": \"replies\"}\n",
    ")\n",
    "\n",
    "embedding_super = SuperComponent(\n",
    "    pipeline=emb_pipeline,\n",
    "    input_mapping={\n",
    "        \"query\": [\"embedder.text\", \"reranker.query\", \"prompt_builder.query\"]\n",
    "    },\n",
    "    output_mapping={\"generator.replies\": \"replies\"}\n",
    ")\n",
    "\n",
    "web_super = SuperComponent(\n",
    "    pipeline=web_pipeline,\n",
    "    input_mapping={\n",
    "        \"query\": [\"search.query\", \"ranker.query\", \"prompt_builder.query\"]\n",
    "    },\n",
    "    output_mapping={\"generator.replies\": \"replies\"}\n",
    ")\n",
    "\n",
    "# define tools based on the wrapped pipelines\n",
    "graph_tool = ComponentTool(\n",
    "    name=\"graph_search\",\n",
    "    component=graph_super,\n",
    "    description=(\n",
    "        \"Answer questions using structured information from a knowledge graph containing factual relationships \"\n",
    "        \"about the European Union’s news, policies, laws, and actions. The graph includes relationships (triples) \"\n",
    "        \"and the original source documents. Answers are grounded in these facts, with references provided as HTML links.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "embedding_tool = ComponentTool(\n",
    "    name=\"embedding_search\",\n",
    "    component=embedding_super,\n",
    "    description=(\n",
    "        \"Answer questions using information retrieved from an internal document store containing content \"\n",
    "        \"about the European Union’s news, policies, laws, and actions. Answers are based strictly on \"\n",
    "        \"retrieved documents using semantic similarity, with no assumptions. References are included as HTML links.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "web_tool = ComponentTool(\n",
    "    name=\"web_search\",\n",
    "    component=web_super,\n",
    "    description=(\n",
    "        \"Retrieve potentially relevant information from the web. Results are based on live internet content and may \"\n",
    "        \"include a variety of sources. The retrieved information is not guaranteed to be factual. References are provided \"\n",
    "        \"as HTML links using either inferred titles or domain names.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# define the agent's system behavior and reasoning instructions\n",
    "system_prompt = \"\"\"\n",
    "You are a highly intelligent assistant with access to 3 specialized tools for answering questions\n",
    "about the European Union’s news, policies, laws, and actions.\n",
    "\n",
    "You have access to:\n",
    "\n",
    "- graph_search: Uses a knowledge graph containing factual relationships (triples) and their source documents.\n",
    "  Answers should be grounded in these structured relationships, using HTML links for citations.\n",
    "\n",
    "- embedding_search: Retrieves semantically relevant information from an internal document store.\n",
    "  Answers must be based strictly on the retrieved documents, using inline HTML links for references.\n",
    "\n",
    "- web_search: Retrieves the most recent and relevant information from the web.\n",
    "  Answers should reflect real-time sources, with references using HTML links. If no title is available,\n",
    "  use the domain name of the URL as the anchor text.\n",
    "\n",
    "Your task:\n",
    "1. Use all three tools to answer the user's query.\n",
    "2. Combine insights from graph_search and embedding_search tools to create a complete and informative response in the Internal Search Answer section.\n",
    "3. Provide separetely inshights from web_search tool to complete the informative response in the Web Search Insights section.\n",
    "3. In each sentece of your answer add the references you were based on.\n",
    "4. Ensure all references are included as inline HTML anchor tags, using titles or domain names as specified.\n",
    "5. If there is a conflict between the information retrieved from the Web Search and the other tools, highlight the discrepancy separetely if there is one in the Conflicts for Internal and Web Search section.\n",
    "6. For any part of the answer generated from web_search too, always clearly indicate that the information comes from the web.\n",
    "7. Output the answer in a structured markdown format.\n",
    "8. Use bullet lists whenever it makes sense.\n",
    "9. Do not add a references section at the end of the answer, just use references within the body of text.\n",
    "\n",
    "Your output should have three sections if there are no conflicts or four sections if there are conflicts:\n",
    "\n",
    "Thought Process:\n",
    "- Describe step-by-step how each tool contributed to your reasoning and answer.\n",
    "\n",
    "Internal Search Answer:\n",
    "- Provide a clear, concise answer supported by insights from graph_search and embedding_search tools, indicating from which tool the answer is based on.\n",
    "\n",
    "Web Search Insights:\n",
    "- Any content derived from a web search must be explicitly identified as such in the response here.\n",
    "\n",
    "Conflicts for Internal and Web Search:\n",
    "- Any conflict of information derived from the internal compared with the web search be explicitly identified as such in the response here.\n",
    "\n",
    "Always include this disclaimer at the end of the final answer:\n",
    "Disclaimer: This is AI generated content — please use it with caution.\n",
    "\"\"\"\n",
    "\n",
    "# create the agent with the toolset and system prompt\n",
    "agent = Agent(\n",
    "    chat_generator=chat_generator,\n",
    "    tools=[embedding_tool, graph_tool, web_tool],\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "# prepare the agent for interaction\n",
    "agent.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKyF8GU31xob"
   },
   "outputs": [],
   "source": [
    "def run_qa_turn(agent, messages, user_input):\n",
    "    \"\"\"\n",
    "    Run a single Q&A turn with the agent.\n",
    "\n",
    "    Appends the user's input to the message history, executes the agent run,\n",
    "    and parses the response into key parts including tool calls, tool outputs,\n",
    "    and the final answer.\n",
    "\n",
    "    Args:\n",
    "        agent (Agent): The config multi-tool agent.\n",
    "        messages (List[ChatMessage]): Conversation history.\n",
    "        user_input (str): The current user input.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[ChatMessage], Dict]: A tuple containing the updated messages list,\n",
    "        and a dictionary with:\n",
    "            - user_input: the last user message as a string\n",
    "            - tool_calls: list of ToolCall objects\n",
    "            - tool_results: mapping of tool names to stringified output\n",
    "            - final_answer: the assistant’s concluding response\n",
    "    \"\"\"\n",
    "    # ddd user input to message history and execute the agent pipeline\n",
    "    messages.append(ChatMessage.from_user(user_input))\n",
    "\n",
    "    result = agent.run(messages=messages, max_steps=10)\n",
    "    messages = result[\"messages\"]\n",
    "\n",
    "    # init output containers\n",
    "    tool_calls = []\n",
    "    tool_results = {}\n",
    "    final_answer = None\n",
    "    last_user_input = None\n",
    "\n",
    "    # parse returned messages\n",
    "    for msg in messages:\n",
    "        role = msg._role.value.lower()\n",
    "        content = msg._content\n",
    "\n",
    "        if role == \"user\" and content and isinstance(content[0], TextContent):\n",
    "            last_user_input = content[0].text\n",
    "\n",
    "        elif role == \"assistant\" and content:\n",
    "            if isinstance(content[0], ToolCall):\n",
    "                tool_calls.extend(content)\n",
    "            elif isinstance(content[0], TextContent):\n",
    "                final_answer = content[0].text\n",
    "\n",
    "        elif role == \"tool\" and content:\n",
    "            for tool_result in content:\n",
    "                if isinstance(tool_result, ToolCallResult):\n",
    "                    tool_name = tool_result.origin.tool_name\n",
    "                    raw = tool_result.result\n",
    "\n",
    "                    try:\n",
    "                        parsed = ast.literal_eval(raw)\n",
    "\n",
    "                        if isinstance(parsed, dict) and \"replies\" in parsed:\n",
    "                            replies = parsed[\"replies\"]\n",
    "                            if isinstance(replies, list):\n",
    "                                reply_text = \"\\n\".join(replies)\n",
    "                            else:\n",
    "                                reply_text = str(replies)\n",
    "                        else:\n",
    "                            reply_text = str(parsed)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        reply_text = raw  # fallback\n",
    "                        print(f\"Failed to parse tool result for {tool_name}: {e}\")\n",
    "\n",
    "                    tool_results[tool_name] = reply_text\n",
    "\n",
    "    response = {\n",
    "        \"user_input\": last_user_input,\n",
    "        \"tool_calls\": tool_calls,\n",
    "        \"tool_results\": tool_results,\n",
    "        \"final_answer\": final_answer\n",
    "    }\n",
    "\n",
    "    return messages, response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HftPL1pHnmhl"
   },
   "source": [
    "#### RAG Evaluation Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSFaG4MzHfXq"
   },
   "source": [
    "##### Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHD4fI8JATyT"
   },
   "outputs": [],
   "source": [
    "def fix_inner_double_quotes_in_single_quoted_values(json_str, verbose=False):\n",
    "    \"\"\"\n",
    "    Replaces any double quotes inside single-quoted string values with single quotes.\n",
    "    Only affects values of the form: ...: '...'\n",
    "    \"\"\"\n",
    "    # this regex finds single-quoted values after a colon and optional whitespace\n",
    "    pattern = re.compile(r\":\\s*'([^']*)'\", re.DOTALL)\n",
    "\n",
    "    def replacer(match):\n",
    "        inner = match.group(1)\n",
    "        fixed = inner.replace('\"', \"'\")\n",
    "        if verbose and inner != fixed:\n",
    "            print(f\"Replaced inner double quotes in value: {inner}\")\n",
    "        return \": '\" + fixed + \"'\"\n",
    "\n",
    "    return pattern.sub(replacer, json_str)\n",
    "\n",
    "def replace_double_quote_pair_near_position(json_str, error_pos, verbose=False):\n",
    "    \"\"\"\n",
    "    Replaces the closest pair of unescaped double quotes near error_pos with single quotes.\n",
    "    Returns the new string.\n",
    "    \"\"\"\n",
    "    chars = list(json_str)\n",
    "    # find the first \" to the left\n",
    "    left = error_pos\n",
    "    while left >= 0 and chars[left] != '\"':\n",
    "        left -= 1\n",
    "    # find the first \" to the right\n",
    "    right = error_pos\n",
    "    while right < len(chars) and chars[right] != '\"':\n",
    "        right += 1\n",
    "    replaced = 0\n",
    "    if left >= 0 and chars[left] == '\"' and (left == 0 or chars[left-1] != '\\\\'):\n",
    "        chars[left] = \"'\"\n",
    "        replaced += 1\n",
    "        if verbose:\n",
    "            print(f\"Replaced \\\" with ' at position {left}\")\n",
    "    if right < len(chars) and chars[right] == '\"' and (right == 0 or chars[right-1] != '\\\\'):\n",
    "        chars[right] = \"'\"\n",
    "        replaced += 1\n",
    "        if verbose:\n",
    "            print(f\"Replaced \\\" with ' at position {right}\")\n",
    "    if replaced == 0 and verbose:\n",
    "        print(\"No double quotes found to replace near error position.\")\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def find_json_block(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the largest valid JSON object block in the text (from first { to last }).\n",
    "    Handles markdown fences, smart quotes, and multi-line LLM output.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"```json|```\", \"\", text)\n",
    "    text = text.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
    "    start, end = text.find('{'), text.rfind('}')\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        candidate = text[start:end+1]\n",
    "        try:\n",
    "            json.loads(candidate)\n",
    "            return candidate\n",
    "        except Exception:\n",
    "            pass\n",
    "    return text  # fallback\n",
    "\n",
    "def unwrap_evaluations(parsed):\n",
    "    \"\"\"\n",
    "    Returns a list of evaluation dicts regardless of LLM output style.\n",
    "    Supports:\n",
    "    - {'evaluations': [ ... ]}\n",
    "    - [ ... ]  # just a list\n",
    "    - { ... }  # a single evaluation dict (rare)\n",
    "    \"\"\"\n",
    "    if isinstance(parsed, dict) and \"evaluations\" in parsed:\n",
    "        return parsed[\"evaluations\"]\n",
    "    elif isinstance(parsed, list):\n",
    "        return parsed\n",
    "    elif isinstance(parsed, dict):  # if just a single dict\n",
    "        # defensive: check for statement/score keys\n",
    "        if any(k in parsed for k in (\"statement\", \"score\", \"justification\")):\n",
    "            return [parsed]\n",
    "        else:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def try_fix_json(json_str: str, verbose: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Attempt to fix common LLM-to-JSON errors:\n",
    "    - Remove markdown/code fences\n",
    "    - Normalize smart quotes\n",
    "    - Remove trailing commas before closing braces\n",
    "    - Balance brackets\n",
    "    - Print error context if requested\n",
    "    \"\"\"\n",
    "    json_str = re.sub(r\"```json|```\", \"\", json_str)\n",
    "    json_str = json_str.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
    "    json_str = re.sub(r',\\s*([}\\]])', r'\\1', json_str)\n",
    "    json_str = re.sub(r'\\n+', '\\n', json_str)\n",
    "    json_str = json_str.replace(\"\\t\", \" \")\n",
    "    while json_str.count('{') > json_str.count('}'):\n",
    "        json_str += '}'\n",
    "    while json_str.count('[') > json_str.count(']'):\n",
    "        json_str += ']'\n",
    "    json_str = re.sub(r',\\s*,', ',', json_str)\n",
    "    json_str = re.sub(r'//.*?\\n', '', json_str)\n",
    "    json_str = json_str.replace('\\ufeff', '')\n",
    "    json_str = re.sub(r'\\bNone\\b', 'null', json_str)\n",
    "    return json_str\n",
    "\n",
    "def parse_llm_json_reply(reply_text: str, verbose: bool = False, unwrap_evaluations: bool = False):\n",
    "    \"\"\"\n",
    "    Tries to extract and parse a JSON object from LLM reply.\n",
    "    Always returns a list of dicts (evaluations), or empty list.\n",
    "    \"\"\"\n",
    "    candidate = find_json_block(reply_text)\n",
    "    parsed = None\n",
    "    try:\n",
    "        parsed = json.loads(candidate)\n",
    "    except json.JSONDecodeError as e:\n",
    "        if verbose:\n",
    "            print(\"First JSON decode failed:\", e)\n",
    "            print(\"Context around error:\", candidate[e.pos-40:e.pos+40])\n",
    "        fixed = try_fix_json(candidate, verbose=verbose)\n",
    "        try:\n",
    "            parsed = json.loads(fixed)\n",
    "        except json.JSONDecodeError as e2:\n",
    "            if verbose:\n",
    "                print(\"Second JSON decode failed:\", e2)\n",
    "                print(\"Context around error:\", fixed[e2.pos-40:e2.pos+40])\n",
    "            try:\n",
    "                parsed = demjson3.decode(fixed)\n",
    "            except Exception as e3:\n",
    "                if verbose:\n",
    "                    print(\"demjson3 also failed:\", e3)\n",
    "                try:\n",
    "                    json.loads(fixed)\n",
    "                except json.JSONDecodeError as e4:\n",
    "                    fixed2 = replace_double_quote_pair_near_position(fixed, e4.pos, verbose=verbose)\n",
    "                    try:\n",
    "                        parsed = json.loads(fixed2)\n",
    "                    except Exception as e5:\n",
    "                        if verbose:\n",
    "                            print(\"replace_double_quote_pair_near_position also failed:\", e5)\n",
    "                        fixed3 = fix_inner_double_quotes_in_single_quoted_values(fixed, verbose=verbose)\n",
    "                        try:\n",
    "                            parsed = json.loads(fixed3)\n",
    "                        except Exception as e6:\n",
    "                            if verbose:\n",
    "                                print(\"fix_inner_double_quotes_in_single_quoted_strings also failed:\", e6)\n",
    "                            return []\n",
    "    if unwrap_evaluations: # for eval\n",
    "        if isinstance(parsed, dict) and \"evaluations\" in parsed:\n",
    "            return parsed[\"evaluations\"]\n",
    "        elif isinstance(parsed, list):\n",
    "            return parsed\n",
    "        elif isinstance(parsed, dict):\n",
    "            if any(k in parsed for k in (\"statement\", \"score\", \"justification\")):\n",
    "                return [parsed]\n",
    "            return []\n",
    "        return []\n",
    "    else:\n",
    "        # for statements extraction, just return the parsed JSON (dict)\n",
    "        return parsed\n",
    "\n",
    "def has_valid_results(results):\n",
    "    \"\"\"\n",
    "    True if 'results' contains at least one entry in its 'results' list\n",
    "    where at least one of statements, statement_scores, justifications is non-empty.\n",
    "    \"\"\"\n",
    "    if not isinstance(results, dict):\n",
    "        return False\n",
    "    results_list = results.get(\"results\")\n",
    "    if not (isinstance(results_list, list) and len(results_list) > 0):\n",
    "        return False\n",
    "    for entry in results_list:\n",
    "        for key in [\"statements\", \"statement_scores\", \"justifications\"]:\n",
    "            val = entry.get(key, [])\n",
    "            if isinstance(val, list) and len(val) > 0:\n",
    "                return True\n",
    "    return False  # all empty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqE1BK6lM2Ae"
   },
   "source": [
    "##### Statements extractor config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfdPg9PlATyU"
   },
   "outputs": [],
   "source": [
    "class StatementsExtractor(Component):\n",
    "    \"\"\"\n",
    "    Extracts factual statements from a generated answer, given the question and answer.\n",
    "    Contexts may be provided for tracking, but are not sent to the LLM.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=\"gpt-4.1\",\n",
    "        provider=\"openai\",\n",
    "        instructions: str = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.provider = provider.lower()\n",
    "        self.model = model\n",
    "\n",
    "        if self.provider == \"openai\":\n",
    "            self.chat_generator = OpenAIChatGenerator(\n",
    "                model=model,\n",
    "                generation_kwargs={\"temperature\": 0, \"top_p\": 0},\n",
    "                api_key=Secret.from_env_var(\"OPENAI_API_KEY\")\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"provider must be 'openai'\")\n",
    "\n",
    "        self.instructions = instructions or (\n",
    "            \"You will receive a question and a LLM-based generated answer.\\n\"\n",
    "            \"Your task is to break the generated answer into a list of factual statements by splitting it into sentences, \"\n",
    "            \"leaving each sentence unchanged and exactly as it appears in the generated answer, only replace double with single quotes if double quotes are present. \"\n",
    "            \"Do not rewrite, paraphrase, or alter the sentences in any way. \"\n",
    "            \"Do not include the Disclaimer: This is AI generated content in the statements. \"\n",
    "            \"Do not include sentences like For more detailed information.., since there are no factual statements. \"\n",
    "            \"Only use information from the answer itself (do NOT use any external knowledge or assumptions).\\n\\n\"\n",
    "            \"All JSON keys and string values must use double quotes (\\\").\\n\"\n",
    "            \"If it is required to quote something inside a statement, use ONLY single quotes (').\\n\"\n",
    "            \"Output only a valid JSON object with the key 'statements', which is a list of strings. Do not add any text before or after the JSON.\\n\"\n",
    "            \"Example:\\n\"\n",
    "            \"{\\n\"\n",
    "            '  \\\"statements\\\": [\\n'\n",
    "            '    \\\"The Clean Industrial Deal is designed to reduce industrial emissions.\\\",\\n'\n",
    "            '    \\\"Affordable energy is the foundation of competitiveness.\\\"\\n'\n",
    "            \"  ]\\n\"\n",
    "            \"}\\n\"\n",
    "            \"If the answer contains no factual statements, output an empty list.\"\n",
    "        )\n",
    "    @component.output_types(\n",
    "        question=str,\n",
    "        generated_answer=str,\n",
    "        statements=List[str],\n",
    "        contexts=Optional[List[str]]\n",
    "    )\n",
    "    def run(\n",
    "        self,\n",
    "        questions: List[str],\n",
    "        generated_answers: List[str],\n",
    "        contexts: Optional[List[List[str]]] = None\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        results = []\n",
    "        num_samples = len(questions)\n",
    "        if contexts is None:\n",
    "            contexts = [None] * num_samples\n",
    "\n",
    "        for question, generated_answer, context_list in zip(questions, generated_answers, contexts):\n",
    "            prompt = (\n",
    "                f\"{self.instructions}\\n\\n\"\n",
    "                f\"Question: {question}\\n\\n\"\n",
    "                f\"Generated Answer:\\n{generated_answer}\\n\\n\"\n",
    "                f'Respond with a JSON object: {{\"statements\": [...]}}'\n",
    "            )\n",
    "\n",
    "            parsed = None\n",
    "            statements_list = []\n",
    "            retries = 3\n",
    "\n",
    "            for attempt in range(retries):\n",
    "                try:\n",
    "                    response = self.chat_generator.run([ChatMessage.from_user(prompt)])\n",
    "                    reply_text = response[\"replies\"][0].text\n",
    "\n",
    "                    parsed = parse_llm_json_reply(reply_text, verbose=True, unwrap_evaluations=False)\n",
    "\n",
    "                    # accept either {\"statements\": [...]} or just a list\n",
    "                    if isinstance(parsed, dict) and \"statements\" in parsed:\n",
    "                        statements_list = parsed[\"statements\"]\n",
    "                        break  # Success!\n",
    "                    elif isinstance(parsed, list):\n",
    "                        # sometimes the model just outputs a list\n",
    "                        statements_list = parsed\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"[Attempt {attempt+1}] Failed: {str(e)}\")\n",
    "                    statements_list = []\n",
    "                    time.sleep(1)\n",
    "\n",
    "            else:\n",
    "                print(\"Failed to get a valid response after retries.\")\n",
    "                statements_list = []\n",
    "\n",
    "            # PATCH insertation if Final answer: inconclusive and there are no statements extracted\n",
    "            if (\n",
    "                (not statements_list or len(statements_list) == 0)\n",
    "                and isinstance(generated_answer, str)\n",
    "                and \"final answer: inconclusive\" in generated_answer.lower()\n",
    "            ):\n",
    "                statements_list = [generated_answer]\n",
    "\n",
    "            result = {\n",
    "                \"question\": question,\n",
    "                \"generated_answer\": generated_answer,\n",
    "                \"statements\": statements_list,\n",
    "                \"contexts\": context_list if context_list is not None else None,\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "        return results\n",
    "\n",
    "# instantiation:\n",
    "statements_extractor = StatementsExtractor(model=\"gpt-4.1\", provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRzKAGauM5eD"
   },
   "source": [
    "##### RAG Faithfulness evaluator config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scMFpKitATyU"
   },
   "outputs": [],
   "source": [
    "class CustomFaithfulnessEvaluator(Component):\n",
    "    \"\"\"\n",
    "    Faithfulness evaluator for QA pipelines, modular for OpenAI or Anthropic providers.\n",
    "    Expects statements to be provided (does not generate them).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model=\"gpt-4.1\",\n",
    "                 provider=\"openai\",\n",
    "                 instructions: str = None):\n",
    "        super().__init__()\n",
    "        self.provider = provider.lower()\n",
    "        self.model = model\n",
    "\n",
    "        if self.provider == \"openai\":\n",
    "            self.chat_generator = OpenAIChatGenerator(\n",
    "                model=model,\n",
    "                generation_kwargs={\"temperature\": 0, \"top_p\": 0}\n",
    "            )\n",
    "        elif self.provider == \"anthropic\":\n",
    "            self.chat_generator = AnthropicChatGenerator(\n",
    "                model=model,\n",
    "                generation_kwargs={\"temperature\": 0, \"max_tokens\": 8192}\n",
    "            )\n",
    "        elif self.provider == \"google\":\n",
    "            self.chat_generator = GoogleAIGeminiChatGenerator(\n",
    "                model=model,\n",
    "                generation_config={\"temperature\": 0}\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"provider must be 'openai' or 'anthropic'\")\n",
    "\n",
    "        self.instructions = instructions or (\n",
    "            \"You are evaluating the faithfulness of a list of factual statements with respect to a provided context.\\n\\n\"\n",
    "            \"You will receive:\\n\"\n",
    "            \"- a question\\n\"\n",
    "            \"- a context: a set of retrieved documents or passages used to generate the answer\\n\"\n",
    "            \"- a list of factual statements (already extracted from a predicted answer)\\n\\n\"\n",
    "            \"TASK:\\n\"\n",
    "            \"For each statement in the list, output an object with:\\n\"\n",
    "            \"   - \\\"statement\\\": the statement, **repeated exactly as given, with no modifications**\\n\"\n",
    "            \"   - \\\"score\\\": (see below)\\n\"\n",
    "            \"   - \\\"justification\\\": a short explanation\\n\\n\"\n",
    "            \"Scoring:\\n\"\n",
    "            \"   a. If the statement is clearly supported by the context → score = 1\\n\"\n",
    "            \"   b. If the statement is not supported or the context is silent → score = 0\\n\"\n",
    "            \"   c. If the statement includes or is equivalent to 'Final Answer: inconclusive' AND this is justified by the lack of support in context → score = -1\\n\"\n",
    "            \"   d. If the statement includes or is equivalent to 'Final Answer: inconclusive' BUT the context does contain sufficient information to answer the question → score = -2\\n\\n\"\n",
    "            \"Formatting instructions:\\n\"\n",
    "            \"- Output a single valid JSON block, and **do not add any text before or after**.\\n\"\n",
    "            \"- Format the output as follows:\\n\"\n",
    "            '{\\n'\n",
    "            '  \\\"evaluations\\\": [\\n'\n",
    "            '    {\\\"statement\\\": \\\"...\\\", \\\"score\\\": 1, \\\"justification\\\": \\\"...\\\"},\\n'\n",
    "            '    {\\\"statement\\\": \\\"...\\\", \\\"score\\\": 1, \\\"justification\\\": \\\"...\\\"}\\n'\n",
    "            '  ]\\n'\n",
    "            '}\\n'\n",
    "            \"- There MUST be one object for each statement, in the same order as given, and each 'statement' field must match the input exactly.\\n\"\n",
    "            \"- If you cannot score a statement, set \\\"score\\\": null and \\\"justification\\\": \\\"MISSING\\\".\\n\\n\"\n",
    "            \"JSON Output Format instructions:\\n\"\n",
    "            \"- **All JSON keys and string values must use double quotes (\\\").**\\n\"\n",
    "            \"- **If you must quote text inside any JSON string value (in either statement or justification), you MUST use single quotes ('), NEVER double quotes (\\\"). Double quotes are reserved for JSON formatting only.**\\n\"\n",
    "            \"- **This also applies for nested quoting; even if you are quoting a phrase inside another quoted phrase. Use single quotes for all levels of quoting/ referencing in the justifications.**\\n\"\n",
    "            \"- Do NOT add any commentary, markdown, or preamble; output only **VALID** JSON as shown above.\\n\\n\"\n",
    "            \"Assess and score **every** statement.\\n\\n\"\n",
    "            \"You should ALWAYS output the same number of statements given, along with their assessment.\\n\"\n",
    "            \"Be sure that you did not miss any given statement.\\n\"\n",
    "        )\n",
    "\n",
    "    @component.output_types(\n",
    "        individual_scores=List[float],\n",
    "        score=float,\n",
    "        results=List[Dict[str, Any]]\n",
    "    )\n",
    "    def run(self, questions: List[str], contexts: List[List[str]], statements: List[List[str]]) -> Dict[str, Any]:\n",
    "        results = []\n",
    "        individual_scores = []\n",
    "\n",
    "        for question, context_list, statements_list in zip(questions, contexts, statements):\n",
    "            full_context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc}\" for i, doc in enumerate(context_list)])\n",
    "            statements_json = json.dumps(statements_list, ensure_ascii=False)\n",
    "            evals = []\n",
    "            error_flag = False\n",
    "\n",
    "            for attempt in range(3):\n",
    "                prompt = (\n",
    "                    f\"{self.instructions}\\n\\n\"\n",
    "                    f\"Question: {question}\\n\\n\"\n",
    "                    f\"Context:\\n{full_context}\\n\\n\"\n",
    "                    f\"Statements:\\n{statements_json}\\n\\n\"\n",
    "                    f'Respond with a JSON object as specified.'\n",
    "                )\n",
    "                try:\n",
    "                    response = self.chat_generator.run([ChatMessage.from_user(prompt)])\n",
    "                    reply_text = response[\"replies\"][0].text\n",
    "\n",
    "                    # coerce reply_text to string if not already\n",
    "                    if isinstance(reply_text, dict):\n",
    "                        if len(reply_text) == 1:\n",
    "                            reply_text = list(reply_text.values())[0]\n",
    "                        else:\n",
    "                            reply_text = json.dumps(reply_text)\n",
    "                    elif not isinstance(reply_text, str):\n",
    "                        reply_text = str(reply_text)\n",
    "\n",
    "                    evals = parse_llm_json_reply(reply_text, verbose=True, unwrap_evaluations=True)\n",
    "                    if len(evals) == len(statements_list):\n",
    "                        # Success!\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Attempt {attempt+1}: Number of output evaluations ({len(evals)}) does not match number of input statements ({len(statements_list)}). Retrying...\")\n",
    "                        time.sleep(1)\n",
    "                except Exception as e:\n",
    "                    print(f\"[Attempt {attempt+1}] Failed: {str(e)}\")\n",
    "                    evals = []\n",
    "                    time.sleep(1)\n",
    "            else:\n",
    "                # all attempts failed or count mismatch\n",
    "                error_flag = True\n",
    "                print(\"Failed to get a valid response with matching statement count after 3 attempts.\")\n",
    "                evals = [{\"statement\": s, \"score\": None, \"justification\": \"MISSING\"} for s in statements_list]\n",
    "\n",
    "            statement_scores = []\n",
    "            justifications = []\n",
    "            output_statements = []\n",
    "\n",
    "            for eval_obj in evals:\n",
    "                output_statements.append(eval_obj.get(\"statement\", \"\"))\n",
    "                statement_scores.append(eval_obj.get(\"score\", None))\n",
    "                justifications.append(eval_obj.get(\"justification\", \"\"))\n",
    "\n",
    "            # compute mean only on non-null scores\n",
    "            numeric_scores = [s for s in statement_scores if isinstance(s, (int, float))]\n",
    "            score = float(np.mean(numeric_scores)) if numeric_scores else 0.0\n",
    "            result_dict = {\n",
    "                \"statements\": output_statements,\n",
    "                \"statement_scores\": statement_scores,\n",
    "                \"justifications\": justifications,\n",
    "                \"score\": score,\n",
    "                \"error\": \"count_mismatch\" if error_flag else None,\n",
    "            }\n",
    "            results.append(result_dict)\n",
    "            individual_scores.append(score)\n",
    "\n",
    "        final_score = float(np.mean(individual_scores)) if individual_scores else 0.0\n",
    "\n",
    "        return {\n",
    "            \"results\": results,\n",
    "            \"individual_scores\": individual_scores,\n",
    "            \"score\": final_score\n",
    "        }\n",
    "\n",
    "# instantiations:\n",
    "standard_evaluator_gpt = CustomFaithfulnessEvaluator(model=\"gpt-4.1\", provider=\"openai\")\n",
    "standard_evaluator_anthropic = CustomFaithfulnessEvaluator(model=\"claude-sonnet-4-0\", provider=\"anthropic\")\n",
    "standard_evaluator_google = CustomFaithfulnessEvaluator(model=\"gemini-2.5-pro\", provider=\"google\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9CJKqYaTinD"
   },
   "source": [
    "#### Standard Pipelines Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8XqlLC4ATyU"
   },
   "source": [
    "##### Generate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0JKOpupATyU"
   },
   "outputs": [],
   "source": [
    "def generate_answers(pipeline_type, pipeline_obj, questions_json_path):\n",
    "    \"\"\"\n",
    "    Generate answers for all questions using the selected pipeline (emb, graph, web).\n",
    "\n",
    "    Args:\n",
    "        pipeline_type: \"emb\", \"graph\", or \"web\"\n",
    "        pipeline_obj:   the corresponding pipeline object\n",
    "        questions_json_path: path to the input JSON with {\"questions\": [...]}\n",
    "    \"\"\"\n",
    "    assert pipeline_type in (\"emb\", \"graph\", \"web\"), \"Unknown pipeline type\"\n",
    "    answers_file = f\"{pipeline_type}_generated_answers.json\"\n",
    "\n",
    "    with open(questions_json_path, \"r\") as f:\n",
    "        questions = json.load(f).get(\"questions\", [])\n",
    "\n",
    "    results = [{} for _ in questions]\n",
    "\n",
    "    for idx, question in enumerate(questions):\n",
    "        print(f\"[{pipeline_type}] Generating answer for question {idx+1}/{len(questions)}\")\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            if pipeline_type == \"emb\":\n",
    "                result = pipeline_obj.run({\n",
    "                    \"embedder\": {\"text\": question},\n",
    "                    \"retriever\": {\"top_k\": 5},\n",
    "                    \"reranker\": {\"query\": question},\n",
    "                    \"prompt_builder\": {\"query\": question}\n",
    "                })\n",
    "            elif pipeline_type == \"graph\":\n",
    "                result = pipeline_obj.run({\n",
    "                    \"kg_retriever\": {\"query\": question},\n",
    "                    \"ranker\": {\"query\": question},\n",
    "                    \"prompt_builder\": {\"query\": question}\n",
    "                })\n",
    "            elif pipeline_type == \"web\":\n",
    "                result = pipeline_obj.run({\n",
    "                    \"search\": {\"query\": question},\n",
    "                    \"ranker\": {\"query\": question},\n",
    "                    \"prompt_builder\": {\"query\": question}\n",
    "                })\n",
    "            else:\n",
    "                raise ValueError(\"Unknown pipeline type\")\n",
    "            end_time = time.time()\n",
    "            latency = end_time - start_time\n",
    "\n",
    "            generated_answer = result[\"generator\"][\"replies\"][0]\n",
    "            contexts = [doc.content for doc in result[\"output_docs\"][\"documents\"]]\n",
    "\n",
    "            entry = {\n",
    "                \"question\": question,\n",
    "                \"generated_answer\": generated_answer,\n",
    "                \"contexts\": contexts,\n",
    "                \"latency_seconds\": latency\n",
    "            }\n",
    "        except Exception as e:\n",
    "            entry = {\n",
    "                \"question\": question,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "        results[idx] = entry\n",
    "\n",
    "        with open(answers_file, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"Results saved to {answers_file}\")\n",
    "\n",
    "# usage\n",
    "generate_answers(\"emb\", emb_pipeline, \"eval/eval_questions.json\")\n",
    "generate_answers(\"graph\", graph_pipeline, \"eval/eval_questions.json\")\n",
    "generate_answers(\"web\", web_pipeline, \"eval/eval_questions.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AUht6acATyU"
   },
   "source": [
    "##### Extract statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWpxU1GXATyV"
   },
   "outputs": [],
   "source": [
    "def extract_statements(answers_file, statements_file, statements_extractor):\n",
    "\n",
    "    \"\"\"\n",
    "    For each entry in answers_file, use statements_extractor to extract statements,\n",
    "    and write the results (with statements) to statements_file.\n",
    "    \"\"\"\n",
    "    with open(answers_file, \"r\") as f:\n",
    "        answer_entries = json.load(f)\n",
    "\n",
    "    # this will hold the enriched entries\n",
    "    statements_entries = answer_entries.copy()\n",
    "\n",
    "    for idx, entry in enumerate(statements_entries):\n",
    "        if \"error\" in entry or \"generated_answer\" not in entry:\n",
    "            continue  # skip failed/empty entries\n",
    "\n",
    "        if \"statements\" in entry:  # already done\n",
    "            continue\n",
    "\n",
    "        print(f\"Extracting statements for question {idx+1}\")\n",
    "\n",
    "        statements_result = statements_extractor.run(\n",
    "            questions=[entry[\"question\"]],\n",
    "            generated_answers=[entry[\"generated_answer\"]],\n",
    "            contexts=[entry.get(\"contexts\", [])]  # for tracking, not sent to LLM\n",
    "        )\n",
    "\n",
    "        entry[\"statements\"] = statements_result[0][\"statements\"]\n",
    "\n",
    "        with open(statements_file, \"w\") as f:\n",
    "            json.dump(statements_entries, f, indent=2)\n",
    "\n",
    "    print(f\"Statements saved to {statements_file}\")\n",
    "\n",
    "# usage\n",
    "extract_statements(\"emb_generated_answers.json\", \"emb_statements.json\", statements_extractor)\n",
    "extract_statements(\"graph_generated_answers.json\", \"graph_statements.json\", statements_extractor)\n",
    "extract_statements(\"web_generated_answers.json\", \"web_statements.json\", statements_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sen3qHHQATyV"
   },
   "source": [
    "##### Faithfulness eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPDHvB5UATyV"
   },
   "outputs": [],
   "source": [
    "def run_faithfulness_eval(statements_file, eval_results_file, evaluators):\n",
    "    \"\"\"\n",
    "    Runs faithfulness evaluation for each entry in the statements_file,\n",
    "    and saves enriched results (with eval outputs) to eval_results_file.\n",
    "\n",
    "    Args:\n",
    "        statements_file: input file with 'statements' for each entry\n",
    "        eval_results_file: output file with evaluator results\n",
    "        evaluators: dict of {\"evaluator_name\": evaluator_object, ...}\n",
    "    \"\"\"\n",
    "    with open(statements_file, \"r\") as f:\n",
    "        answer_entries = json.load(f)\n",
    "\n",
    "    results_entries = answer_entries.copy()\n",
    "\n",
    "    for idx, entry in enumerate(results_entries):\n",
    "        if \"error\" in entry or \"statements\" not in entry or not entry[\"statements\"]:\n",
    "            continue  # skip if no statements\n",
    "\n",
    "        # only run if not already done for all requested evaluators\n",
    "        skip = True\n",
    "        for name in evaluators:\n",
    "            if f\"results_{name}\" not in entry:\n",
    "                skip = False\n",
    "        if skip:\n",
    "            continue\n",
    "\n",
    "        print(f\"Evaluating faithfulness for question {idx+1}\")\n",
    "\n",
    "        questions = [entry[\"question\"]]\n",
    "        contexts = [[c for c in entry.get(\"contexts\", []) if isinstance(c, str) and c.strip()]]\n",
    "        statements = [entry[\"statements\"]]\n",
    "\n",
    "\n",
    "        for eval_name, eval_obj in evaluators.items():\n",
    "            key = f\"results_{eval_name}\"\n",
    "            if key in entry:\n",
    "                continue\n",
    "            result = eval_obj.run(\n",
    "                questions=questions,\n",
    "                contexts=contexts,\n",
    "                statements=statements\n",
    "            )\n",
    "            entry[key] = result\n",
    "\n",
    "        with open(eval_results_file, \"w\") as f:\n",
    "            json.dump(results_entries, f, indent=2)\n",
    "\n",
    "    print(f\"Faithfulness eval results saved to {eval_results_file}\")\n",
    "\n",
    "# usage\n",
    "# define which evaluators you want to use (any subset)\n",
    "evaluators = {\n",
    "    \"gpt\": standard_evaluator_gpt,\n",
    "    \"anthropic\": standard_evaluator_anthropic,\n",
    "    \"google\": standard_evaluator_google,\n",
    "}\n",
    "\n",
    "run_faithfulness_eval(\"emb_statements.json\", \"emb_faithfulness_evaluation_results.json\", evaluators)\n",
    "run_faithfulness_eval(\"graph_statements.json\", \"graph_faithfulness_evaluation_results.json\", evaluators)\n",
    "run_faithfulness_eval(\"web_statements.json\", \"web_faithfulness_evaluation_results.json\", evaluators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KU3lXS5gATyV"
   },
   "source": [
    "#### Agent Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YHN5HFUATyV"
   },
   "source": [
    "##### Agent evaluator config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3OYltl1ATyV"
   },
   "outputs": [],
   "source": [
    "class CustomAgentFaithfulnessEvaluator(Component):\n",
    "    \"\"\"\n",
    "    Faithfulness and attribution evaluator for agent answers, robust and standard-output style.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model=\"gpt-4.1\",\n",
    "                 provider=\"openai\",\n",
    "                 instructions: str = None):\n",
    "        super().__init__()\n",
    "        self.provider = provider.lower()\n",
    "        self.model = model\n",
    "\n",
    "        if self.provider == \"openai\":\n",
    "            self.chat_generator = OpenAIChatGenerator(\n",
    "                model=model,\n",
    "                generation_kwargs={\"temperature\": 0, \"top_p\": 0}\n",
    "            )\n",
    "        elif self.provider == \"anthropic\":\n",
    "            self.chat_generator = AnthropicChatGenerator(\n",
    "                model=model,\n",
    "                generation_kwargs={\"temperature\": 0, \"max_tokens\": 8192}\n",
    "            )\n",
    "        elif self.provider == \"google\":\n",
    "            self.chat_generator = GoogleAIGeminiChatGenerator(\n",
    "                model=model,\n",
    "                generation_config={\"temperature\": 0}\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"provider must be 'openai' or 'anthropic'\")\n",
    "\n",
    "        self.instructions = instructions or (\n",
    "            \"You are evaluating the faithfulness and attribution of a list of factual statements, given specific tool contexts and a predicted answer from an agent.\\n\\n\"\n",
    "            \"You will receive:\\n\"\n",
    "            \"- a question\\n\"\n",
    "            \"- embedding_search_context: information from internal knowledge from the embedding_search tool\\n\"\n",
    "            \"- graph_search_context: information from internal knowledge from the graph_search tool\\n\"\n",
    "            \"- web_search_context: information retrieved from the web_search tool\\n\"\n",
    "            \"- a predicted answer from an agent\\n\"\n",
    "            \"- a list of factual statements (already extracted from the answer; do NOT extract statements yourself)\\n\\n\"\n",
    "            \"The predicted answer you receive SHOULD include:\\n\"\n",
    "            \"   - an Internal Search Answer section, produced with embedding_search and graph_search tool results\\n\"\n",
    "            \"   - a Web Search Insights section, produced with web_search tool results\\n\\n\"\n",
    "            \"Your task is:\\n\"\n",
    "            \"For each provided statement, output an object with:\\n\"\n",
    "            \"   - \\\"statement\\\": the statement, **repeated exactly as given, with no modifications**\\n\"\n",
    "            \"   - \\\"score\\\": (see detailed rubric below)\\n\"\n",
    "            \"   - \\\"justification\\\": a short explanation citing which context(s) support the statement and why that score applies\\n\\n\"\n",
    "            \"Scoring rubric:\\n\"\n",
    "            \"   a. If the statement is supported ONLY by embedding_search_context:\\n\"\n",
    "            \"      → score = 1\\n\"\n",
    "            \"      justification: reference the specific content from embedding_search_context that supports the statement\\n\\n\"\n",
    "            \"   b. If the statement is supported ONLY by graph_search_context:\\n\"\n",
    "            \"      → score = 2\\n\"\n",
    "            \"      justification: reference the specific content from graph_search_context that supports the statement\\n\\n\"\n",
    "            \"   c. If the statement is supported by BOTH embedding_search_context and graph_search_context:\\n\"\n",
    "            \"      → score = 3\\n\"\n",
    "            \"      justification: show support from both contexts and explain how they jointly confirm the statement\\n\\n\"\n",
    "            \"   d. If the statement is a conflict statement AND is correctly identified because of conflict information from embedding_search_context or graph_search_context compared with web_search_context AND is clearly indicated/marked as conflict:\\n\"\n",
    "            \"      → score = 4\\n\"\n",
    "            \"      justification: describe the conflict and how the answer clearly signals it as such\\n\\n\"\n",
    "            \"   e. If the statement is a conflict statement BUT is incorrectly identified (because there is no actual conflict in the contexts) AND is clearly indicated/marked as conflict:\\n\"\n",
    "            \"      → score = 5\\n\"\n",
    "            \"      justification: note the false marking of conflict and explain how the sources do not disagree\\n\\n\"\n",
    "            \"   f. If the statement is supported by embedding_search_context or graph_search_context, but there is a conflict with web_search_context AND it is NOT clearly indicated/marked as conflict:\\n\"\n",
    "            \"      → score = 6\\n\"\n",
    "            \"      justification: explain the internal vs. web disagreement and the lack of conflict signaling in the answer\\n\\n\"\n",
    "            \"   g. If there is a conflict between internal (embedding or graph) and web context BUT it is NOT clearly indicated/marked as conflict:\\n\"\n",
    "            \"      → score = 7\\n\"\n",
    "            \"      justification: highlight the missed conflict and explain how it should have been addressed\\n\\n\"\n",
    "            \"   h. If the statement reflects an explicit indication that no conflicts were found:\\n\"\n",
    "            \"      → score = 8\\n\"\n",
    "            \"      justification: explain how the answer claims consensus and the contexts indeed show agreement\\n\\n\"\n",
    "            \"   i. If the statement is not supported by any context:\\n\"\n",
    "            \"      → score = 0\\n\"\n",
    "            \"      justification: explain the absence of supporting evidence in all contexts\\n\\n\"\n",
    "            \"   j. If the statement is supported by web_search_context AND is clearly indicated/marked as coming from web search in the answer:\\n\"\n",
    "            \"      → score = 9\\n\"\n",
    "            \"      justification: show the web-based support and cite how it was properly marked as web-sourced\\n\\n\"\n",
    "            \"   k. If the statement is supported by web_search_context BUT is NOT clearly indicated/marked as from the web:\\n\"\n",
    "            \"      → score = -9\\n\"\n",
    "            \"      justification: provide evidence from the web_search_context and explain how the answer failed to mark it as web-sourced\\n\\n\"\n",
    "            \"Formatting instructions:\\n\"\n",
    "            \"- Output a single valid JSON block, and **do not add any text before or after**.\\n\"\n",
    "            \"- Format the output as follows:\\n\"\n",
    "            \"{\\n\"\n",
    "            '  \\\"evaluations\\\": [\\n'\n",
    "            '    {\\\"statement\\\": \\\"...\\\", \\\"score\\\": 1, \\\"justification\\\": \\\"...\\\"},\\n'\n",
    "            '    {\\\"statement\\\": \\\"...\\\", \\\"score\\\": 2, \\\"justification\\\": \\\"...\\\"}\\n'\n",
    "            '  ]\\n'\n",
    "            \"}\\n\"\n",
    "            \"- There MUST be one object for each statement, in the same order as given, and each 'statement' field must match the input exactly.\\n\"\n",
    "            \"- If you cannot score a statement, set \\\"score\\\": null and \\\"justification\\\": \\\"MISSING\\\".\\n\"\n",
    "            \"- **All JSON keys and string values must use double quotes (\\\").**\\n\"\n",
    "            \"- **If you need to quote text inside a string value, use only single quotes (').**\\n\"\n",
    "            \"- Assess and score **every** statement.\\n\"\n",
    "            \"- Do NOT add any commentary, markdown, or preamble; output only valid JSON as shown above.\\n\\n\"\n",
    "        )\n",
    "\n",
    "    @component.output_types(\n",
    "        individual_scores=List[float],\n",
    "        score=float,\n",
    "        results=List[Dict[str, Any]]\n",
    "    )\n",
    "    def run(\n",
    "        self,\n",
    "        questions: List[str],\n",
    "        predicted_answers: List[str],\n",
    "        emb_contexts: List[List[str]],\n",
    "        graph_contexts: List[List[str]],\n",
    "        web_search_contexts: List[List[str]],\n",
    "        statements: List[List[str]],\n",
    "    ) -> Dict[str, Any]:\n",
    "        import numpy as np\n",
    "        results = []\n",
    "        individual_scores = []\n",
    "\n",
    "        for question, emb_retrieved, graph_retrieved, web_search, stmts, answer in zip(\n",
    "            questions, emb_contexts, graph_contexts, web_search_contexts, statements, predicted_answers\n",
    "        ):\n",
    "            full_emb = \" \".join(emb_retrieved)\n",
    "            full_graph = \" \".join(graph_retrieved)\n",
    "            full_web = \" \".join(web_search)\n",
    "            statements_json = json.dumps(stmts, ensure_ascii=False, indent=2)\n",
    "\n",
    "            # Short-circuit if answer is inconclusive (optional)\n",
    "            if \"final answer: inconclusive\" in answer.lower():\n",
    "                result_dict = {\n",
    "                    \"statements\": [],\n",
    "                    \"statement_scores\": [],\n",
    "                    \"justifications\": [],\n",
    "                    \"score\": 0.0,\n",
    "                    \"error\": \"inconclusive\"\n",
    "                }\n",
    "                results.append(result_dict)\n",
    "                individual_scores.append(0.0)\n",
    "                continue\n",
    "\n",
    "            prompt = (\n",
    "                f\"{self.instructions}\\n\\n\"\n",
    "                f\"Question:\\n{question}\\n\\n\"\n",
    "                f\"embedding_search_context:\\n{full_emb}\\n\\n\"\n",
    "                f\"graph_search_context:\\n{full_graph}\\n\\n\"\n",
    "                f\"web_search_context:\\n{full_web}\\n\\n\"\n",
    "                f\"Predicted Answer:\\n{answer}\\n\"\n",
    "                f\"Statements:\\n{statements_json}\\n\\n\"\n",
    "                f\"Respond with a JSON object as specified.\"\n",
    "            )\n",
    "\n",
    "            statements_list = []\n",
    "            scores_list = []\n",
    "            justifications_list = []\n",
    "            error_flag = False\n",
    "\n",
    "            for attempt in range(3):\n",
    "                try:\n",
    "                    response = self.chat_generator.run([ChatMessage.from_user(prompt)])\n",
    "                    reply_text = response[\"replies\"][0].text\n",
    "                    parsed = parse_llm_json_reply(reply_text, verbose=True, unwrap_evaluations=True)\n",
    "\n",
    "                    # Accept either {\"statements\": [...]} or just a list of dicts (rare)\n",
    "                    if isinstance(parsed, dict) and \"statements\" in parsed:\n",
    "                        statements_list = parsed.get(\"statements\", [])\n",
    "                        scores_list = parsed.get(\"statement_scores\", [])\n",
    "                        justifications_list = parsed.get(\"justifications\", [])\n",
    "                        if len(statements_list) == len(scores_list) == len(justifications_list):\n",
    "                            break\n",
    "                    elif isinstance(parsed, list):\n",
    "                        # Accept a list of dicts (rare LLM fail)\n",
    "                        statements_list = [item.get(\"statement\", \"\") for item in parsed]\n",
    "                        scores_list = [item.get(\"score\", None) for item in parsed]\n",
    "                        justifications_list = [item.get(\"justification\", \"\") for item in parsed]\n",
    "                        if len(statements_list) == len(scores_list) == len(justifications_list):\n",
    "                            break\n",
    "                except Exception as e:\n",
    "                    print(f\"Attempt {attempt+1} failed: {str(e)}\")\n",
    "                    statements_list = []\n",
    "                    scores_list = []\n",
    "                    justifications_list = []\n",
    "                    time.sleep(1)\n",
    "            else:\n",
    "                error_flag = True\n",
    "                print(\"Failed to get valid response after retries.\")\n",
    "                # ff nothing at all, fill with empty\n",
    "                statements_list = []\n",
    "                scores_list = []\n",
    "                justifications_list = []\n",
    "\n",
    "            # compute mean only on non-null scores\n",
    "            numeric_scores = [s for s in scores_list if isinstance(s, (int, float))]\n",
    "            mean_score = float(np.mean(numeric_scores)) if numeric_scores else 0.0\n",
    "            result_dict = {\n",
    "                \"statements\": statements_list,\n",
    "                \"statement_scores\": scores_list,\n",
    "                \"justifications\": justifications_list,\n",
    "                \"score\": mean_score,\n",
    "                \"error\": \"count_mismatch\" if error_flag else None,\n",
    "\n",
    "            }\n",
    "            results.append(result_dict)\n",
    "            individual_scores.append(mean_score)\n",
    "\n",
    "        final_score = float(np.mean(individual_scores)) if individual_scores else 0.0\n",
    "\n",
    "        return {\n",
    "            \"results\": results,\n",
    "            \"individual_scores\": individual_scores,\n",
    "            \"score\": final_score\n",
    "        }\n",
    "\n",
    "# instantiations:\n",
    "agent_evaluator_gpt = CustomAgentFaithfulnessEvaluator(model=\"gpt-4.1\", provider=\"openai\")\n",
    "agent_evaluator_anthropic = CustomAgentFaithfulnessEvaluator(model=\"claude-sonnet-4-0\", provider=\"anthropic\")\n",
    "agent_evaluator_google = CustomAgentFaithfulnessEvaluator(model=\"gemini-2.5-pro\", provider=\"google\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXUoqNbJATyV"
   },
   "source": [
    "##### Generate agentic answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEGhXx9LATyV"
   },
   "outputs": [],
   "source": [
    "def generate_agentic_answers(agent, questions_json_path, results_file=\"agent_generated_answers.json\"):\n",
    "    # load questions\n",
    "    with open(questions_json_path, \"r\") as f:\n",
    "        questions = json.load(f).get(\"questions\", [])\n",
    "\n",
    "    # load previous results if any\n",
    "    if os.path.exists(results_file):\n",
    "        with open(results_file, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "    else:\n",
    "        results = [{} for _ in questions]\n",
    "\n",
    "    for idx, question in enumerate(questions):\n",
    "        if results[idx] and \"generated_answer\" in results[idx]:\n",
    "            continue\n",
    "        print(f\"[agent] Generating answer for question {idx+1}/{len(questions)}\")\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            conversation_state = {}\n",
    "            user_id = \"default\"\n",
    "            messages = conversation_state.get(user_id, [])\n",
    "\n",
    "            messages, result = run_qa_turn(agent, messages, question)\n",
    "            conversation_state[user_id] = messages\n",
    "\n",
    "            tool_results = result.get(\"tool_results\", {})\n",
    "            answer = result.get(\"final_answer\", \"No answer generated.\")\n",
    "            if isinstance(answer, tuple):  # sometimes it's a tuple\n",
    "                answer = answer[0]\n",
    "\n",
    "            # contexts (should be list-of-string for each)\n",
    "            emb_context = tool_results.get(\"embedding_search\", [])\n",
    "            if not isinstance(emb_context, list):\n",
    "                emb_context = [emb_context]\n",
    "            graph_context = tool_results.get(\"graph_search\", [])\n",
    "            if not isinstance(graph_context, list):\n",
    "                graph_context = [graph_context]\n",
    "            web_context = tool_results.get(\"web_search\", [])\n",
    "            if not isinstance(web_context, list):\n",
    "                web_context = [web_context]\n",
    "\n",
    "            latency = time.time() - start_time\n",
    "\n",
    "            entry = {\n",
    "                \"question\": question,\n",
    "                \"generated_answer\": answer,\n",
    "                \"emb_based_context\": emb_context,\n",
    "                \"graph_based_context\": graph_context,\n",
    "\n",
    "                \"web_based_context\": web_context,\n",
    "                \"latency_seconds\": latency\n",
    "            }\n",
    "        except Exception as e:\n",
    "            entry = {\n",
    "                \"question\": question,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "        results[idx] = entry\n",
    "\n",
    "        with open(results_file, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "    print(f\"Agentic results saved to {results_file}\")\n",
    "\n",
    "# sage ---\n",
    "generate_agentic_answers(agent, \"eval/eval_questions.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1VqkHGSATyV"
   },
   "source": [
    "##### Extract agentic statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcePdN2eATyV"
   },
   "outputs": [],
   "source": [
    "def extract_statements_agentic(answers_file, statements_file, statements_extractor):\n",
    "    with open(answers_file, \"r\") as f:\n",
    "        answer_entries = json.load(f)\n",
    "    statements_entries = answer_entries.copy()\n",
    "\n",
    "    for idx, entry in enumerate(statements_entries):\n",
    "        if \"error\" in entry or \"generated_answer\" not in entry:\n",
    "            continue\n",
    "        if \"statements\" in entry:\n",
    "            continue\n",
    "\n",
    "        print(f\"Extracting statements for agentic question {idx+1}\")\n",
    "\n",
    "        statements_result = statements_extractor.run(\n",
    "            questions=[entry[\"question\"]],\n",
    "            generated_answers=[entry[\"generated_answer\"]],\n",
    "            contexts=[[]]  # or you could pass some tracking info, but it's not sent to LLM\n",
    "        )\n",
    "\n",
    "        entry[\"statements\"] = statements_result[0][\"statements\"]\n",
    "\n",
    "        # attach contexts for later evaluation (not used by extractor)\n",
    "        entry[\"emb_based_context\"] = entry.get(\"emb_based_context\", [])\n",
    "        entry[\"graph_based_context\"] = entry.get(\"graph_based_context\", [])\n",
    "        entry[\"web_based_context\"] = entry.get(\"web_based_context\", [])\n",
    "\n",
    "        with open(statements_file, \"w\") as f:\n",
    "            json.dump(statements_entries, f, indent=2)\n",
    "\n",
    "    print(f\"Statements saved to {statements_file}\")\n",
    "\n",
    "# usage:\n",
    "extract_statements_agentic(\"agent_generated_answers.json\", \"agent_statements.json\", statements_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mx8FjpXCATyV"
   },
   "source": [
    "##### Faithfulness agentic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AsDMMqTdATyV"
   },
   "outputs": [],
   "source": [
    "def run_agentic_faithfulness_eval(statements_file, eval_results_file, evaluators):\n",
    "    \"\"\"\n",
    "    Runs agentic faithfulness evaluation for each entry in the statements_file,\n",
    "    and saves enriched results (with eval outputs) to eval_results_file.\n",
    "\n",
    "    Args:\n",
    "        statements_file: input file with 'statements' and all contexts for each entry\n",
    "        eval_results_file: output file with agentic evaluator results\n",
    "        evaluators: dict of {\"evaluator_name\": evaluator_object, ...}\n",
    "    \"\"\"\n",
    "    with open(statements_file, \"r\") as f:\n",
    "        answer_entries = json.load(f)\n",
    "\n",
    "    results_entries = answer_entries.copy()\n",
    "\n",
    "    for idx, entry in enumerate(results_entries):\n",
    "        if (\n",
    "            \"error\" in entry\n",
    "            or \"statements\" not in entry\n",
    "            or not entry[\"statements\"]\n",
    "        ):\n",
    "            continue  # skip if no statements\n",
    "\n",
    "        # only run if not already done for all requested evaluators\n",
    "        skip = True\n",
    "        for name in evaluators:\n",
    "            if f\"results_{name}\" not in entry:\n",
    "                skip = False\n",
    "        if skip:\n",
    "            continue\n",
    "\n",
    "        print(f\"Evaluating agentic faithfulness for question {idx+1}\")\n",
    "\n",
    "        questions = [entry[\"question\"]]\n",
    "        predicted_answers = [entry[\"generated_answer\"]]\n",
    "        emb_contexts = [entry.get(\"emb_based_context\", [])]\n",
    "        graph_contexts = [entry.get(\"graph_based_context\", [])]\n",
    "        web_search_contexts = [entry.get(\"web_based_context\", [])]\n",
    "        statements = [entry[\"statements\"]]\n",
    "\n",
    "        for eval_name, eval_obj in evaluators.items():\n",
    "            key = f\"results_{eval_name}\"\n",
    "            if key in entry:\n",
    "                continue\n",
    "            result = eval_obj.run(\n",
    "                questions=questions,\n",
    "                predicted_answers=predicted_answers,\n",
    "                emb_contexts=emb_contexts,\n",
    "                graph_contexts=graph_contexts,\n",
    "                web_search_contexts=web_search_contexts,\n",
    "                statements=statements\n",
    "            )\n",
    "            entry[key] = result\n",
    "\n",
    "        with open(eval_results_file, \"w\") as f:\n",
    "            json.dump(results_entries, f, indent=2)\n",
    "\n",
    "    print(f\"Agentic faithfulness eval results saved to {eval_results_file}\")\n",
    "\n",
    "# usage\n",
    "\n",
    "# define your agent evaluators (use your instantiated ones)\n",
    "agent_evaluators = {\n",
    "    \"gpt\": agent_evaluator_gpt,\n",
    "    \"anthropic\": agent_evaluator_anthropic,\n",
    "    \"google\": agent_evaluator_google,\n",
    "}\n",
    "\n",
    "run_agentic_faithfulness_eval(\n",
    "    \"agent_statements.json\",\n",
    "    \"agent_faithfulness_evaluation_results.json\",\n",
    "    agent_evaluators\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "CIAcD3G4C5xM",
    "vlBA-fAZDKtu",
    "kyJjq0u0QJt0",
    "mSFaG4MzHfXq",
    "pqE1BK6lM2Ae",
    "Q1VqkHGSATyV"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
